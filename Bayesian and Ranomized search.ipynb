{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neo Reference ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Absolute Magnitude</th>\n",
       "      <th>Est Dia in KM(min)</th>\n",
       "      <th>Est Dia in KM(max)</th>\n",
       "      <th>Est Dia in M(min)</th>\n",
       "      <th>Est Dia in M(max)</th>\n",
       "      <th>Est Dia in Miles(min)</th>\n",
       "      <th>Est Dia in Miles(max)</th>\n",
       "      <th>Est Dia in Feet(min)</th>\n",
       "      <th>...</th>\n",
       "      <th>Asc Node Longitude</th>\n",
       "      <th>Orbital Period</th>\n",
       "      <th>Perihelion Distance</th>\n",
       "      <th>Perihelion Arg</th>\n",
       "      <th>Aphelion Dist</th>\n",
       "      <th>Perihelion Time</th>\n",
       "      <th>Mean Anomaly</th>\n",
       "      <th>Mean Motion</th>\n",
       "      <th>Equinox</th>\n",
       "      <th>Hazardous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3703080</td>\n",
       "      <td>3703080</td>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>...</td>\n",
       "      <td>314.373913</td>\n",
       "      <td>609.599786</td>\n",
       "      <td>0.808259</td>\n",
       "      <td>57.257470</td>\n",
       "      <td>2.005764</td>\n",
       "      <td>2.458162e+06</td>\n",
       "      <td>264.837533</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>J2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3723955</td>\n",
       "      <td>3723955</td>\n",
       "      <td>21.300</td>\n",
       "      <td>0.146068</td>\n",
       "      <td>0.326618</td>\n",
       "      <td>146.067964</td>\n",
       "      <td>326.617897</td>\n",
       "      <td>0.090762</td>\n",
       "      <td>0.202951</td>\n",
       "      <td>479.225620</td>\n",
       "      <td>...</td>\n",
       "      <td>136.717242</td>\n",
       "      <td>425.869294</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>313.091975</td>\n",
       "      <td>1.497352</td>\n",
       "      <td>2.457795e+06</td>\n",
       "      <td>173.741112</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2446862</td>\n",
       "      <td>2446862</td>\n",
       "      <td>20.300</td>\n",
       "      <td>0.231502</td>\n",
       "      <td>0.517654</td>\n",
       "      <td>231.502122</td>\n",
       "      <td>517.654482</td>\n",
       "      <td>0.143849</td>\n",
       "      <td>0.321655</td>\n",
       "      <td>759.521423</td>\n",
       "      <td>...</td>\n",
       "      <td>259.475979</td>\n",
       "      <td>643.580228</td>\n",
       "      <td>0.950791</td>\n",
       "      <td>248.415038</td>\n",
       "      <td>1.966857</td>\n",
       "      <td>2.458120e+06</td>\n",
       "      <td>292.893654</td>\n",
       "      <td>0.559371</td>\n",
       "      <td>J2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3092506</td>\n",
       "      <td>3092506</td>\n",
       "      <td>27.400</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>8.801465</td>\n",
       "      <td>19.680675</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>28.876199</td>\n",
       "      <td>...</td>\n",
       "      <td>57.173266</td>\n",
       "      <td>514.082140</td>\n",
       "      <td>0.983902</td>\n",
       "      <td>18.707701</td>\n",
       "      <td>1.527904</td>\n",
       "      <td>2.457902e+06</td>\n",
       "      <td>68.741007</td>\n",
       "      <td>0.700277</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3514799</td>\n",
       "      <td>3514799</td>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>...</td>\n",
       "      <td>84.629307</td>\n",
       "      <td>495.597821</td>\n",
       "      <td>0.967687</td>\n",
       "      <td>158.263596</td>\n",
       "      <td>1.483543</td>\n",
       "      <td>2.457814e+06</td>\n",
       "      <td>135.142133</td>\n",
       "      <td>0.726395</td>\n",
       "      <td>J2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>3759007</td>\n",
       "      <td>3759007</td>\n",
       "      <td>23.900</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>0.098637</td>\n",
       "      <td>44.111820</td>\n",
       "      <td>98.637028</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>144.723824</td>\n",
       "      <td>...</td>\n",
       "      <td>164.183305</td>\n",
       "      <td>457.179984</td>\n",
       "      <td>0.741558</td>\n",
       "      <td>276.395697</td>\n",
       "      <td>1.581299</td>\n",
       "      <td>2.457708e+06</td>\n",
       "      <td>304.306025</td>\n",
       "      <td>0.787436</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>3759295</td>\n",
       "      <td>3759295</td>\n",
       "      <td>28.200</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>6.089126</td>\n",
       "      <td>13.615700</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>19.977449</td>\n",
       "      <td>...</td>\n",
       "      <td>345.225230</td>\n",
       "      <td>407.185767</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>42.111064</td>\n",
       "      <td>1.153835</td>\n",
       "      <td>2.458088e+06</td>\n",
       "      <td>282.978786</td>\n",
       "      <td>0.884117</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>3759714</td>\n",
       "      <td>3759714</td>\n",
       "      <td>22.700</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.171412</td>\n",
       "      <td>76.657557</td>\n",
       "      <td>171.411509</td>\n",
       "      <td>0.047633</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>251.501180</td>\n",
       "      <td>...</td>\n",
       "      <td>37.026468</td>\n",
       "      <td>690.054279</td>\n",
       "      <td>0.965760</td>\n",
       "      <td>274.692712</td>\n",
       "      <td>2.090708</td>\n",
       "      <td>2.458300e+06</td>\n",
       "      <td>203.501147</td>\n",
       "      <td>0.521698</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>3759720</td>\n",
       "      <td>3759720</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0.116026</td>\n",
       "      <td>0.259442</td>\n",
       "      <td>116.025908</td>\n",
       "      <td>259.441818</td>\n",
       "      <td>0.072095</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>380.662441</td>\n",
       "      <td>...</td>\n",
       "      <td>163.802910</td>\n",
       "      <td>662.048343</td>\n",
       "      <td>1.185467</td>\n",
       "      <td>180.346090</td>\n",
       "      <td>1.787733</td>\n",
       "      <td>2.458288e+06</td>\n",
       "      <td>203.524965</td>\n",
       "      <td>0.543767</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>3772978</td>\n",
       "      <td>3772978</td>\n",
       "      <td>19.109</td>\n",
       "      <td>0.400641</td>\n",
       "      <td>0.895860</td>\n",
       "      <td>400.640618</td>\n",
       "      <td>895.859655</td>\n",
       "      <td>0.248946</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>1314.437764</td>\n",
       "      <td>...</td>\n",
       "      <td>187.642183</td>\n",
       "      <td>653.679098</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>222.436688</td>\n",
       "      <td>2.071980</td>\n",
       "      <td>2.458319e+06</td>\n",
       "      <td>184.820424</td>\n",
       "      <td>0.550729</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4687 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Neo Reference ID     Name  Absolute Magnitude  Est Dia in KM(min)  \\\n",
       "0              3703080  3703080              21.600            0.127220   \n",
       "1              3723955  3723955              21.300            0.146068   \n",
       "2              2446862  2446862              20.300            0.231502   \n",
       "3              3092506  3092506              27.400            0.008801   \n",
       "4              3514799  3514799              21.600            0.127220   \n",
       "...                ...      ...                 ...                 ...   \n",
       "4682           3759007  3759007              23.900            0.044112   \n",
       "4683           3759295  3759295              28.200            0.006089   \n",
       "4684           3759714  3759714              22.700            0.076658   \n",
       "4685           3759720  3759720              21.800            0.116026   \n",
       "4686           3772978  3772978              19.109            0.400641   \n",
       "\n",
       "      Est Dia in KM(max)  Est Dia in M(min)  Est Dia in M(max)  \\\n",
       "0               0.284472         127.219879         284.472297   \n",
       "1               0.326618         146.067964         326.617897   \n",
       "2               0.517654         231.502122         517.654482   \n",
       "3               0.019681           8.801465          19.680675   \n",
       "4               0.284472         127.219879         284.472297   \n",
       "...                  ...                ...                ...   \n",
       "4682            0.098637          44.111820          98.637028   \n",
       "4683            0.013616           6.089126          13.615700   \n",
       "4684            0.171412          76.657557         171.411509   \n",
       "4685            0.259442         116.025908         259.441818   \n",
       "4686            0.895860         400.640618         895.859655   \n",
       "\n",
       "      Est Dia in Miles(min)  Est Dia in Miles(max)  Est Dia in Feet(min)  ...  \\\n",
       "0                  0.079051               0.176763            417.388066  ...   \n",
       "1                  0.090762               0.202951            479.225620  ...   \n",
       "2                  0.143849               0.321655            759.521423  ...   \n",
       "3                  0.005469               0.012229             28.876199  ...   \n",
       "4                  0.079051               0.176763            417.388066  ...   \n",
       "...                     ...                    ...                   ...  ...   \n",
       "4682               0.027410               0.061290            144.723824  ...   \n",
       "4683               0.003784               0.008460             19.977449  ...   \n",
       "4684               0.047633               0.106510            251.501180  ...   \n",
       "4685               0.072095               0.161210            380.662441  ...   \n",
       "4686               0.248946               0.556661           1314.437764  ...   \n",
       "\n",
       "      Asc Node Longitude Orbital Period  Perihelion Distance  Perihelion Arg  \\\n",
       "0             314.373913     609.599786             0.808259       57.257470   \n",
       "1             136.717242     425.869294             0.718200      313.091975   \n",
       "2             259.475979     643.580228             0.950791      248.415038   \n",
       "3              57.173266     514.082140             0.983902       18.707701   \n",
       "4              84.629307     495.597821             0.967687      158.263596   \n",
       "...                  ...            ...                  ...             ...   \n",
       "4682          164.183305     457.179984             0.741558      276.395697   \n",
       "4683          345.225230     407.185767             0.996434       42.111064   \n",
       "4684           37.026468     690.054279             0.965760      274.692712   \n",
       "4685          163.802910     662.048343             1.185467      180.346090   \n",
       "4686          187.642183     653.679098             0.876110      222.436688   \n",
       "\n",
       "      Aphelion Dist  Perihelion Time  Mean Anomaly  Mean Motion  Equinox  \\\n",
       "0          2.005764     2.458162e+06    264.837533     0.590551    J2000   \n",
       "1          1.497352     2.457795e+06    173.741112     0.845330    J2000   \n",
       "2          1.966857     2.458120e+06    292.893654     0.559371    J2000   \n",
       "3          1.527904     2.457902e+06     68.741007     0.700277    J2000   \n",
       "4          1.483543     2.457814e+06    135.142133     0.726395    J2000   \n",
       "...             ...              ...           ...          ...      ...   \n",
       "4682       1.581299     2.457708e+06    304.306025     0.787436    J2000   \n",
       "4683       1.153835     2.458088e+06    282.978786     0.884117    J2000   \n",
       "4684       2.090708     2.458300e+06    203.501147     0.521698    J2000   \n",
       "4685       1.787733     2.458288e+06    203.524965     0.543767    J2000   \n",
       "4686       2.071980     2.458319e+06    184.820424     0.550729    J2000   \n",
       "\n",
       "      Hazardous  \n",
       "0          True  \n",
       "1         False  \n",
       "2          True  \n",
       "3         False  \n",
       "4          True  \n",
       "...         ...  \n",
       "4682      False  \n",
       "4683      False  \n",
       "4684      False  \n",
       "4685      False  \n",
       "4686      False  \n",
       "\n",
       "[4687 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('nasa.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4687 entries, 0 to 4686\n",
      "Data columns (total 40 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Neo Reference ID              4687 non-null   int64  \n",
      " 1   Name                          4687 non-null   int64  \n",
      " 2   Absolute Magnitude            4687 non-null   float64\n",
      " 3   Est Dia in KM(min)            4687 non-null   float64\n",
      " 4   Est Dia in KM(max)            4687 non-null   float64\n",
      " 5   Est Dia in M(min)             4687 non-null   float64\n",
      " 6   Est Dia in M(max)             4687 non-null   float64\n",
      " 7   Est Dia in Miles(min)         4687 non-null   float64\n",
      " 8   Est Dia in Miles(max)         4687 non-null   float64\n",
      " 9   Est Dia in Feet(min)          4687 non-null   float64\n",
      " 10  Est Dia in Feet(max)          4687 non-null   float64\n",
      " 11  Close Approach Date           4687 non-null   object \n",
      " 12  Epoch Date Close Approach     4687 non-null   int64  \n",
      " 13  Relative Velocity km per sec  4687 non-null   float64\n",
      " 14  Relative Velocity km per hr   4687 non-null   float64\n",
      " 15  Miles per hour                4687 non-null   float64\n",
      " 16  Miss Dist.(Astronomical)      4687 non-null   float64\n",
      " 17  Miss Dist.(lunar)             4687 non-null   float64\n",
      " 18  Miss Dist.(kilometers)        4687 non-null   float64\n",
      " 19  Miss Dist.(miles)             4687 non-null   float64\n",
      " 20  Orbiting Body                 4687 non-null   object \n",
      " 21  Orbit ID                      4687 non-null   int64  \n",
      " 22  Orbit Determination Date      4687 non-null   object \n",
      " 23  Orbit Uncertainity            4687 non-null   int64  \n",
      " 24  Minimum Orbit Intersection    4687 non-null   float64\n",
      " 25  Jupiter Tisserand Invariant   4687 non-null   float64\n",
      " 26  Epoch Osculation              4687 non-null   float64\n",
      " 27  Eccentricity                  4687 non-null   float64\n",
      " 28  Semi Major Axis               4687 non-null   float64\n",
      " 29  Inclination                   4687 non-null   float64\n",
      " 30  Asc Node Longitude            4687 non-null   float64\n",
      " 31  Orbital Period                4687 non-null   float64\n",
      " 32  Perihelion Distance           4687 non-null   float64\n",
      " 33  Perihelion Arg                4687 non-null   float64\n",
      " 34  Aphelion Dist                 4687 non-null   float64\n",
      " 35  Perihelion Time               4687 non-null   float64\n",
      " 36  Mean Anomaly                  4687 non-null   float64\n",
      " 37  Mean Motion                   4687 non-null   float64\n",
      " 38  Equinox                       4687 non-null   object \n",
      " 39  Hazardous                     4687 non-null   bool   \n",
      "dtypes: bool(1), float64(30), int64(5), object(4)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neo Reference ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Absolute Magnitude</th>\n",
       "      <th>Est Dia in KM(min)</th>\n",
       "      <th>Est Dia in KM(max)</th>\n",
       "      <th>Est Dia in M(min)</th>\n",
       "      <th>Est Dia in M(max)</th>\n",
       "      <th>Est Dia in Miles(min)</th>\n",
       "      <th>Est Dia in Miles(max)</th>\n",
       "      <th>Est Dia in Feet(min)</th>\n",
       "      <th>...</th>\n",
       "      <th>Semi Major Axis</th>\n",
       "      <th>Inclination</th>\n",
       "      <th>Asc Node Longitude</th>\n",
       "      <th>Orbital Period</th>\n",
       "      <th>Perihelion Distance</th>\n",
       "      <th>Perihelion Arg</th>\n",
       "      <th>Aphelion Dist</th>\n",
       "      <th>Perihelion Time</th>\n",
       "      <th>Mean Anomaly</th>\n",
       "      <th>Mean Motion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.687000e+03</td>\n",
       "      <td>4.687000e+03</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4.687000e+03</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.272298e+06</td>\n",
       "      <td>3.272298e+06</td>\n",
       "      <td>22.267865</td>\n",
       "      <td>0.204604</td>\n",
       "      <td>0.457509</td>\n",
       "      <td>204.604203</td>\n",
       "      <td>457.508906</td>\n",
       "      <td>0.127135</td>\n",
       "      <td>0.284283</td>\n",
       "      <td>671.273653</td>\n",
       "      <td>...</td>\n",
       "      <td>1.400264</td>\n",
       "      <td>13.373844</td>\n",
       "      <td>172.157275</td>\n",
       "      <td>635.582076</td>\n",
       "      <td>0.813383</td>\n",
       "      <td>183.932151</td>\n",
       "      <td>1.987144</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>181.167927</td>\n",
       "      <td>0.738242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.486011e+05</td>\n",
       "      <td>5.486011e+05</td>\n",
       "      <td>2.890972</td>\n",
       "      <td>0.369573</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>369.573402</td>\n",
       "      <td>826.391249</td>\n",
       "      <td>0.229642</td>\n",
       "      <td>0.513496</td>\n",
       "      <td>1212.511199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524154</td>\n",
       "      <td>10.936227</td>\n",
       "      <td>103.276777</td>\n",
       "      <td>370.954727</td>\n",
       "      <td>0.242059</td>\n",
       "      <td>103.513035</td>\n",
       "      <td>0.951519</td>\n",
       "      <td>9.442264e+02</td>\n",
       "      <td>107.501623</td>\n",
       "      <td>0.342627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000433e+06</td>\n",
       "      <td>2.000433e+06</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>1.010543</td>\n",
       "      <td>2.259644</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>3.315431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615920</td>\n",
       "      <td>0.014513</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>176.557161</td>\n",
       "      <td>0.080744</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.803765</td>\n",
       "      <td>2.450100e+06</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.086285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.097594e+06</td>\n",
       "      <td>3.097594e+06</td>\n",
       "      <td>20.100000</td>\n",
       "      <td>0.033462</td>\n",
       "      <td>0.074824</td>\n",
       "      <td>33.462237</td>\n",
       "      <td>74.823838</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>0.046493</td>\n",
       "      <td>109.784247</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000635</td>\n",
       "      <td>4.962341</td>\n",
       "      <td>83.081208</td>\n",
       "      <td>365.605031</td>\n",
       "      <td>0.630834</td>\n",
       "      <td>95.625916</td>\n",
       "      <td>1.266059</td>\n",
       "      <td>2.457815e+06</td>\n",
       "      <td>87.006918</td>\n",
       "      <td>0.453289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.514799e+06</td>\n",
       "      <td>3.514799e+06</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>0.110804</td>\n",
       "      <td>0.247765</td>\n",
       "      <td>110.803882</td>\n",
       "      <td>247.765013</td>\n",
       "      <td>0.068850</td>\n",
       "      <td>0.153954</td>\n",
       "      <td>363.529809</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240981</td>\n",
       "      <td>10.311836</td>\n",
       "      <td>172.625393</td>\n",
       "      <td>504.947292</td>\n",
       "      <td>0.833153</td>\n",
       "      <td>189.761641</td>\n",
       "      <td>1.618195</td>\n",
       "      <td>2.457973e+06</td>\n",
       "      <td>185.718889</td>\n",
       "      <td>0.712946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.690060e+06</td>\n",
       "      <td>3.690060e+06</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>0.253837</td>\n",
       "      <td>0.567597</td>\n",
       "      <td>253.837029</td>\n",
       "      <td>567.596853</td>\n",
       "      <td>0.157727</td>\n",
       "      <td>0.352688</td>\n",
       "      <td>832.798679</td>\n",
       "      <td>...</td>\n",
       "      <td>1.678364</td>\n",
       "      <td>19.511681</td>\n",
       "      <td>255.026909</td>\n",
       "      <td>794.195972</td>\n",
       "      <td>0.997227</td>\n",
       "      <td>271.777557</td>\n",
       "      <td>2.451171</td>\n",
       "      <td>2.458108e+06</td>\n",
       "      <td>276.531946</td>\n",
       "      <td>0.984669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.781897e+06</td>\n",
       "      <td>3.781897e+06</td>\n",
       "      <td>32.100000</td>\n",
       "      <td>15.579552</td>\n",
       "      <td>34.836938</td>\n",
       "      <td>15579.552413</td>\n",
       "      <td>34836.938254</td>\n",
       "      <td>9.680682</td>\n",
       "      <td>21.646663</td>\n",
       "      <td>51114.018738</td>\n",
       "      <td>...</td>\n",
       "      <td>5.072008</td>\n",
       "      <td>75.406667</td>\n",
       "      <td>359.905890</td>\n",
       "      <td>4172.231343</td>\n",
       "      <td>1.299832</td>\n",
       "      <td>359.993098</td>\n",
       "      <td>8.983852</td>\n",
       "      <td>2.458839e+06</td>\n",
       "      <td>359.917991</td>\n",
       "      <td>2.039000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Neo Reference ID          Name  Absolute Magnitude  Est Dia in KM(min)  \\\n",
       "count      4.687000e+03  4.687000e+03         4687.000000         4687.000000   \n",
       "mean       3.272298e+06  3.272298e+06           22.267865            0.204604   \n",
       "std        5.486011e+05  5.486011e+05            2.890972            0.369573   \n",
       "min        2.000433e+06  2.000433e+06           11.160000            0.001011   \n",
       "25%        3.097594e+06  3.097594e+06           20.100000            0.033462   \n",
       "50%        3.514799e+06  3.514799e+06           21.900000            0.110804   \n",
       "75%        3.690060e+06  3.690060e+06           24.500000            0.253837   \n",
       "max        3.781897e+06  3.781897e+06           32.100000           15.579552   \n",
       "\n",
       "       Est Dia in KM(max)  Est Dia in M(min)  Est Dia in M(max)  \\\n",
       "count         4687.000000        4687.000000        4687.000000   \n",
       "mean             0.457509         204.604203         457.508906   \n",
       "std              0.826391         369.573402         826.391249   \n",
       "min              0.002260           1.010543           2.259644   \n",
       "25%              0.074824          33.462237          74.823838   \n",
       "50%              0.247765         110.803882         247.765013   \n",
       "75%              0.567597         253.837029         567.596853   \n",
       "max             34.836938       15579.552413       34836.938254   \n",
       "\n",
       "       Est Dia in Miles(min)  Est Dia in Miles(max)  Est Dia in Feet(min)  \\\n",
       "count            4687.000000            4687.000000           4687.000000   \n",
       "mean                0.127135               0.284283            671.273653   \n",
       "std                 0.229642               0.513496           1212.511199   \n",
       "min                 0.000628               0.001404              3.315431   \n",
       "25%                 0.020792               0.046493            109.784247   \n",
       "50%                 0.068850               0.153954            363.529809   \n",
       "75%                 0.157727               0.352688            832.798679   \n",
       "max                 9.680682              21.646663          51114.018738   \n",
       "\n",
       "       ...  Semi Major Axis  Inclination  Asc Node Longitude  Orbital Period  \\\n",
       "count  ...      4687.000000  4687.000000         4687.000000     4687.000000   \n",
       "mean   ...         1.400264    13.373844          172.157275      635.582076   \n",
       "std    ...         0.524154    10.936227          103.276777      370.954727   \n",
       "min    ...         0.615920     0.014513            0.001941      176.557161   \n",
       "25%    ...         1.000635     4.962341           83.081208      365.605031   \n",
       "50%    ...         1.240981    10.311836          172.625393      504.947292   \n",
       "75%    ...         1.678364    19.511681          255.026909      794.195972   \n",
       "max    ...         5.072008    75.406667          359.905890     4172.231343   \n",
       "\n",
       "       Perihelion Distance  Perihelion Arg  Aphelion Dist  Perihelion Time  \\\n",
       "count          4687.000000     4687.000000    4687.000000     4.687000e+03   \n",
       "mean              0.813383      183.932151       1.987144     2.457728e+06   \n",
       "std               0.242059      103.513035       0.951519     9.442264e+02   \n",
       "min               0.080744        0.006918       0.803765     2.450100e+06   \n",
       "25%               0.630834       95.625916       1.266059     2.457815e+06   \n",
       "50%               0.833153      189.761641       1.618195     2.457973e+06   \n",
       "75%               0.997227      271.777557       2.451171     2.458108e+06   \n",
       "max               1.299832      359.993098       8.983852     2.458839e+06   \n",
       "\n",
       "       Mean Anomaly  Mean Motion  \n",
       "count   4687.000000  4687.000000  \n",
       "mean     181.167927     0.738242  \n",
       "std      107.501623     0.342627  \n",
       "min        0.003191     0.086285  \n",
       "25%       87.006918     0.453289  \n",
       "50%      185.718889     0.712946  \n",
       "75%      276.531946     0.984669  \n",
       "max      359.917991     2.039000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=df.columns.str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neo_reference_id</th>\n",
       "      <th>name</th>\n",
       "      <th>absolute_magnitude</th>\n",
       "      <th>est_dia_in_km(min)</th>\n",
       "      <th>est_dia_in_km(max)</th>\n",
       "      <th>est_dia_in_m(min)</th>\n",
       "      <th>est_dia_in_m(max)</th>\n",
       "      <th>est_dia_in_miles(min)</th>\n",
       "      <th>est_dia_in_miles(max)</th>\n",
       "      <th>est_dia_in_feet(min)</th>\n",
       "      <th>...</th>\n",
       "      <th>asc_node_longitude</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>perihelion_distance</th>\n",
       "      <th>perihelion_arg</th>\n",
       "      <th>aphelion_dist</th>\n",
       "      <th>perihelion_time</th>\n",
       "      <th>mean_anomaly</th>\n",
       "      <th>mean_motion</th>\n",
       "      <th>equinox</th>\n",
       "      <th>hazardous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3703080</td>\n",
       "      <td>3703080</td>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>...</td>\n",
       "      <td>314.373913</td>\n",
       "      <td>609.599786</td>\n",
       "      <td>0.808259</td>\n",
       "      <td>57.257470</td>\n",
       "      <td>2.005764</td>\n",
       "      <td>2.458162e+06</td>\n",
       "      <td>264.837533</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>J2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3723955</td>\n",
       "      <td>3723955</td>\n",
       "      <td>21.300</td>\n",
       "      <td>0.146068</td>\n",
       "      <td>0.326618</td>\n",
       "      <td>146.067964</td>\n",
       "      <td>326.617897</td>\n",
       "      <td>0.090762</td>\n",
       "      <td>0.202951</td>\n",
       "      <td>479.225620</td>\n",
       "      <td>...</td>\n",
       "      <td>136.717242</td>\n",
       "      <td>425.869294</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>313.091975</td>\n",
       "      <td>1.497352</td>\n",
       "      <td>2.457795e+06</td>\n",
       "      <td>173.741112</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2446862</td>\n",
       "      <td>2446862</td>\n",
       "      <td>20.300</td>\n",
       "      <td>0.231502</td>\n",
       "      <td>0.517654</td>\n",
       "      <td>231.502122</td>\n",
       "      <td>517.654482</td>\n",
       "      <td>0.143849</td>\n",
       "      <td>0.321655</td>\n",
       "      <td>759.521423</td>\n",
       "      <td>...</td>\n",
       "      <td>259.475979</td>\n",
       "      <td>643.580228</td>\n",
       "      <td>0.950791</td>\n",
       "      <td>248.415038</td>\n",
       "      <td>1.966857</td>\n",
       "      <td>2.458120e+06</td>\n",
       "      <td>292.893654</td>\n",
       "      <td>0.559371</td>\n",
       "      <td>J2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3092506</td>\n",
       "      <td>3092506</td>\n",
       "      <td>27.400</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>8.801465</td>\n",
       "      <td>19.680675</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>28.876199</td>\n",
       "      <td>...</td>\n",
       "      <td>57.173266</td>\n",
       "      <td>514.082140</td>\n",
       "      <td>0.983902</td>\n",
       "      <td>18.707701</td>\n",
       "      <td>1.527904</td>\n",
       "      <td>2.457902e+06</td>\n",
       "      <td>68.741007</td>\n",
       "      <td>0.700277</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3514799</td>\n",
       "      <td>3514799</td>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>...</td>\n",
       "      <td>84.629307</td>\n",
       "      <td>495.597821</td>\n",
       "      <td>0.967687</td>\n",
       "      <td>158.263596</td>\n",
       "      <td>1.483543</td>\n",
       "      <td>2.457814e+06</td>\n",
       "      <td>135.142133</td>\n",
       "      <td>0.726395</td>\n",
       "      <td>J2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>3759007</td>\n",
       "      <td>3759007</td>\n",
       "      <td>23.900</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>0.098637</td>\n",
       "      <td>44.111820</td>\n",
       "      <td>98.637028</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>144.723824</td>\n",
       "      <td>...</td>\n",
       "      <td>164.183305</td>\n",
       "      <td>457.179984</td>\n",
       "      <td>0.741558</td>\n",
       "      <td>276.395697</td>\n",
       "      <td>1.581299</td>\n",
       "      <td>2.457708e+06</td>\n",
       "      <td>304.306025</td>\n",
       "      <td>0.787436</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>3759295</td>\n",
       "      <td>3759295</td>\n",
       "      <td>28.200</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>6.089126</td>\n",
       "      <td>13.615700</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>19.977449</td>\n",
       "      <td>...</td>\n",
       "      <td>345.225230</td>\n",
       "      <td>407.185767</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>42.111064</td>\n",
       "      <td>1.153835</td>\n",
       "      <td>2.458088e+06</td>\n",
       "      <td>282.978786</td>\n",
       "      <td>0.884117</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>3759714</td>\n",
       "      <td>3759714</td>\n",
       "      <td>22.700</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.171412</td>\n",
       "      <td>76.657557</td>\n",
       "      <td>171.411509</td>\n",
       "      <td>0.047633</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>251.501180</td>\n",
       "      <td>...</td>\n",
       "      <td>37.026468</td>\n",
       "      <td>690.054279</td>\n",
       "      <td>0.965760</td>\n",
       "      <td>274.692712</td>\n",
       "      <td>2.090708</td>\n",
       "      <td>2.458300e+06</td>\n",
       "      <td>203.501147</td>\n",
       "      <td>0.521698</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>3759720</td>\n",
       "      <td>3759720</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0.116026</td>\n",
       "      <td>0.259442</td>\n",
       "      <td>116.025908</td>\n",
       "      <td>259.441818</td>\n",
       "      <td>0.072095</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>380.662441</td>\n",
       "      <td>...</td>\n",
       "      <td>163.802910</td>\n",
       "      <td>662.048343</td>\n",
       "      <td>1.185467</td>\n",
       "      <td>180.346090</td>\n",
       "      <td>1.787733</td>\n",
       "      <td>2.458288e+06</td>\n",
       "      <td>203.524965</td>\n",
       "      <td>0.543767</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>3772978</td>\n",
       "      <td>3772978</td>\n",
       "      <td>19.109</td>\n",
       "      <td>0.400641</td>\n",
       "      <td>0.895860</td>\n",
       "      <td>400.640618</td>\n",
       "      <td>895.859655</td>\n",
       "      <td>0.248946</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>1314.437764</td>\n",
       "      <td>...</td>\n",
       "      <td>187.642183</td>\n",
       "      <td>653.679098</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>222.436688</td>\n",
       "      <td>2.071980</td>\n",
       "      <td>2.458319e+06</td>\n",
       "      <td>184.820424</td>\n",
       "      <td>0.550729</td>\n",
       "      <td>J2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4687 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      neo_reference_id     name  absolute_magnitude  est_dia_in_km(min)  \\\n",
       "0              3703080  3703080              21.600            0.127220   \n",
       "1              3723955  3723955              21.300            0.146068   \n",
       "2              2446862  2446862              20.300            0.231502   \n",
       "3              3092506  3092506              27.400            0.008801   \n",
       "4              3514799  3514799              21.600            0.127220   \n",
       "...                ...      ...                 ...                 ...   \n",
       "4682           3759007  3759007              23.900            0.044112   \n",
       "4683           3759295  3759295              28.200            0.006089   \n",
       "4684           3759714  3759714              22.700            0.076658   \n",
       "4685           3759720  3759720              21.800            0.116026   \n",
       "4686           3772978  3772978              19.109            0.400641   \n",
       "\n",
       "      est_dia_in_km(max)  est_dia_in_m(min)  est_dia_in_m(max)  \\\n",
       "0               0.284472         127.219879         284.472297   \n",
       "1               0.326618         146.067964         326.617897   \n",
       "2               0.517654         231.502122         517.654482   \n",
       "3               0.019681           8.801465          19.680675   \n",
       "4               0.284472         127.219879         284.472297   \n",
       "...                  ...                ...                ...   \n",
       "4682            0.098637          44.111820          98.637028   \n",
       "4683            0.013616           6.089126          13.615700   \n",
       "4684            0.171412          76.657557         171.411509   \n",
       "4685            0.259442         116.025908         259.441818   \n",
       "4686            0.895860         400.640618         895.859655   \n",
       "\n",
       "      est_dia_in_miles(min)  est_dia_in_miles(max)  est_dia_in_feet(min)  ...  \\\n",
       "0                  0.079051               0.176763            417.388066  ...   \n",
       "1                  0.090762               0.202951            479.225620  ...   \n",
       "2                  0.143849               0.321655            759.521423  ...   \n",
       "3                  0.005469               0.012229             28.876199  ...   \n",
       "4                  0.079051               0.176763            417.388066  ...   \n",
       "...                     ...                    ...                   ...  ...   \n",
       "4682               0.027410               0.061290            144.723824  ...   \n",
       "4683               0.003784               0.008460             19.977449  ...   \n",
       "4684               0.047633               0.106510            251.501180  ...   \n",
       "4685               0.072095               0.161210            380.662441  ...   \n",
       "4686               0.248946               0.556661           1314.437764  ...   \n",
       "\n",
       "      asc_node_longitude orbital_period  perihelion_distance  perihelion_arg  \\\n",
       "0             314.373913     609.599786             0.808259       57.257470   \n",
       "1             136.717242     425.869294             0.718200      313.091975   \n",
       "2             259.475979     643.580228             0.950791      248.415038   \n",
       "3              57.173266     514.082140             0.983902       18.707701   \n",
       "4              84.629307     495.597821             0.967687      158.263596   \n",
       "...                  ...            ...                  ...             ...   \n",
       "4682          164.183305     457.179984             0.741558      276.395697   \n",
       "4683          345.225230     407.185767             0.996434       42.111064   \n",
       "4684           37.026468     690.054279             0.965760      274.692712   \n",
       "4685          163.802910     662.048343             1.185467      180.346090   \n",
       "4686          187.642183     653.679098             0.876110      222.436688   \n",
       "\n",
       "      aphelion_dist  perihelion_time  mean_anomaly  mean_motion  equinox  \\\n",
       "0          2.005764     2.458162e+06    264.837533     0.590551    J2000   \n",
       "1          1.497352     2.457795e+06    173.741112     0.845330    J2000   \n",
       "2          1.966857     2.458120e+06    292.893654     0.559371    J2000   \n",
       "3          1.527904     2.457902e+06     68.741007     0.700277    J2000   \n",
       "4          1.483543     2.457814e+06    135.142133     0.726395    J2000   \n",
       "...             ...              ...           ...          ...      ...   \n",
       "4682       1.581299     2.457708e+06    304.306025     0.787436    J2000   \n",
       "4683       1.153835     2.458088e+06    282.978786     0.884117    J2000   \n",
       "4684       2.090708     2.458300e+06    203.501147     0.521698    J2000   \n",
       "4685       1.787733     2.458288e+06    203.524965     0.543767    J2000   \n",
       "4686       2.071980     2.458319e+06    184.820424     0.550729    J2000   \n",
       "\n",
       "      hazardous  \n",
       "0          True  \n",
       "1         False  \n",
       "2          True  \n",
       "3         False  \n",
       "4          True  \n",
       "...         ...  \n",
       "4682      False  \n",
       "4683      False  \n",
       "4684      False  \n",
       "4685      False  \n",
       "4686      False  \n",
       "\n",
       "[4687 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neo_reference_id</th>\n",
       "      <th>name</th>\n",
       "      <th>absolute_magnitude</th>\n",
       "      <th>est_dia_in_km(min)</th>\n",
       "      <th>est_dia_in_km(max)</th>\n",
       "      <th>est_dia_in_m(min)</th>\n",
       "      <th>est_dia_in_m(max)</th>\n",
       "      <th>est_dia_in_miles(min)</th>\n",
       "      <th>est_dia_in_miles(max)</th>\n",
       "      <th>est_dia_in_feet(min)</th>\n",
       "      <th>...</th>\n",
       "      <th>inclination</th>\n",
       "      <th>asc_node_longitude</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>perihelion_distance</th>\n",
       "      <th>perihelion_arg</th>\n",
       "      <th>aphelion_dist</th>\n",
       "      <th>perihelion_time</th>\n",
       "      <th>mean_anomaly</th>\n",
       "      <th>mean_motion</th>\n",
       "      <th>hazardous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neo_reference_id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602381</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175461</td>\n",
       "      <td>-0.026381</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.130486</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.051685</td>\n",
       "      <td>-0.020719</td>\n",
       "      <td>-0.269028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602381</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175461</td>\n",
       "      <td>-0.026381</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.130486</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.051685</td>\n",
       "      <td>-0.020719</td>\n",
       "      <td>-0.269028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute_magnitude</th>\n",
       "      <td>0.602381</td>\n",
       "      <td>0.602381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459632</td>\n",
       "      <td>-0.011470</td>\n",
       "      <td>-0.206774</td>\n",
       "      <td>0.086966</td>\n",
       "      <td>0.031784</td>\n",
       "      <td>-0.256169</td>\n",
       "      <td>-0.115855</td>\n",
       "      <td>-0.049401</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>-0.325522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_km(min)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_km(max)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_m(min)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_m(max)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_miles(min)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_miles(max)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_feet(min)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est_dia_in_feet(max)</th>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.499821</td>\n",
       "      <td>-0.613482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch_date_close_approach</th>\n",
       "      <td>0.186513</td>\n",
       "      <td>0.186513</td>\n",
       "      <td>0.168621</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>-0.094121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067337</td>\n",
       "      <td>-0.019341</td>\n",
       "      <td>0.130175</td>\n",
       "      <td>0.131854</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.114616</td>\n",
       "      <td>-0.015533</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>-0.137663</td>\n",
       "      <td>-0.079020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_velocity_km_per_sec</th>\n",
       "      <td>-0.165032</td>\n",
       "      <td>-0.165032</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514657</td>\n",
       "      <td>-0.021301</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>-0.506978</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.144782</td>\n",
       "      <td>0.020006</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.191970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_velocity_km_per_hr</th>\n",
       "      <td>-0.165032</td>\n",
       "      <td>-0.165032</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514657</td>\n",
       "      <td>-0.021301</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>-0.506978</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.144782</td>\n",
       "      <td>0.020006</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.191970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miles_per_hour</th>\n",
       "      <td>-0.165032</td>\n",
       "      <td>-0.165032</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514657</td>\n",
       "      <td>-0.021301</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>-0.506978</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.144782</td>\n",
       "      <td>0.020006</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.191970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miss_dist.(astronomical)</th>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.339117</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255828</td>\n",
       "      <td>-0.023033</td>\n",
       "      <td>-0.109888</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>-0.021483</td>\n",
       "      <td>-0.103231</td>\n",
       "      <td>0.189409</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>0.104642</td>\n",
       "      <td>0.032407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miss_dist.(lunar)</th>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.339117</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255828</td>\n",
       "      <td>-0.023033</td>\n",
       "      <td>-0.109888</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>-0.021483</td>\n",
       "      <td>-0.103231</td>\n",
       "      <td>0.189409</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>0.104642</td>\n",
       "      <td>0.032407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miss_dist.(kilometers)</th>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.339117</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255828</td>\n",
       "      <td>-0.023033</td>\n",
       "      <td>-0.109888</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>-0.021483</td>\n",
       "      <td>-0.103231</td>\n",
       "      <td>0.189409</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>0.104642</td>\n",
       "      <td>0.032407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miss_dist.(miles)</th>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.155782</td>\n",
       "      <td>-0.339117</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255828</td>\n",
       "      <td>-0.023033</td>\n",
       "      <td>-0.109888</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>-0.021483</td>\n",
       "      <td>-0.103231</td>\n",
       "      <td>0.189409</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>0.104642</td>\n",
       "      <td>0.032407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbit_id</th>\n",
       "      <td>-0.651200</td>\n",
       "      <td>-0.651200</td>\n",
       "      <td>-0.575668</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>0.724089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112125</td>\n",
       "      <td>0.047959</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>-0.063884</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.022269</td>\n",
       "      <td>0.037901</td>\n",
       "      <td>0.048985</td>\n",
       "      <td>-0.008898</td>\n",
       "      <td>0.247369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbit_uncertainity</th>\n",
       "      <td>0.611205</td>\n",
       "      <td>0.611205</td>\n",
       "      <td>0.677764</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>-0.399488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228806</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>0.047535</td>\n",
       "      <td>0.107946</td>\n",
       "      <td>0.016307</td>\n",
       "      <td>0.020908</td>\n",
       "      <td>-0.325784</td>\n",
       "      <td>-0.059396</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>-0.328721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_orbit_intersection</th>\n",
       "      <td>-0.158673</td>\n",
       "      <td>-0.158673</td>\n",
       "      <td>-0.488235</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439867</td>\n",
       "      <td>-0.008963</td>\n",
       "      <td>0.279885</td>\n",
       "      <td>0.299991</td>\n",
       "      <td>-0.029130</td>\n",
       "      <td>0.242023</td>\n",
       "      <td>0.080592</td>\n",
       "      <td>-0.013336</td>\n",
       "      <td>-0.290538</td>\n",
       "      <td>-0.288949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jupiter_tisserand_invariant</th>\n",
       "      <td>-0.004854</td>\n",
       "      <td>-0.004854</td>\n",
       "      <td>0.238702</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>-0.133582</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037366</td>\n",
       "      <td>0.018413</td>\n",
       "      <td>-0.893517</td>\n",
       "      <td>-0.537884</td>\n",
       "      <td>0.064496</td>\n",
       "      <td>-0.887879</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.030972</td>\n",
       "      <td>0.992680</td>\n",
       "      <td>-0.003404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch_osculation</th>\n",
       "      <td>0.006023</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>-0.116087</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015879</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>-0.056653</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>-0.006703</td>\n",
       "      <td>-0.063625</td>\n",
       "      <td>0.977613</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.045812</td>\n",
       "      <td>0.040940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eccentricity</th>\n",
       "      <td>-0.125071</td>\n",
       "      <td>-0.125071</td>\n",
       "      <td>-0.361359</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039018</td>\n",
       "      <td>-0.015413</td>\n",
       "      <td>0.548521</td>\n",
       "      <td>-0.412612</td>\n",
       "      <td>-0.003210</td>\n",
       "      <td>0.701294</td>\n",
       "      <td>-0.064366</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>-0.394860</td>\n",
       "      <td>0.183269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semi_major_axis</th>\n",
       "      <td>0.035865</td>\n",
       "      <td>0.035865</td>\n",
       "      <td>-0.212437</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>0.121224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030325</td>\n",
       "      <td>-0.011073</td>\n",
       "      <td>0.995248</td>\n",
       "      <td>0.496847</td>\n",
       "      <td>-0.048999</td>\n",
       "      <td>0.975326</td>\n",
       "      <td>-0.059303</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>-0.901396</td>\n",
       "      <td>-0.010770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inclination</th>\n",
       "      <td>-0.175461</td>\n",
       "      <td>-0.175461</td>\n",
       "      <td>-0.459632</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>0.259450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>-0.046215</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>-0.021653</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.015743</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.009607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asc_node_longitude</th>\n",
       "      <td>-0.026381</td>\n",
       "      <td>-0.026381</td>\n",
       "      <td>-0.011470</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009580</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.020012</td>\n",
       "      <td>-0.012245</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.029477</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.017536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbital_period</th>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>-0.206774</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>-0.009580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.467209</td>\n",
       "      <td>-0.044507</td>\n",
       "      <td>0.977630</td>\n",
       "      <td>-0.058549</td>\n",
       "      <td>-0.025304</td>\n",
       "      <td>-0.859462</td>\n",
       "      <td>-0.011168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perihelion_distance</th>\n",
       "      <td>0.130486</td>\n",
       "      <td>0.130486</td>\n",
       "      <td>0.086966</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>-0.071866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046215</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.467209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053090</td>\n",
       "      <td>0.292995</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>-0.047114</td>\n",
       "      <td>-0.601118</td>\n",
       "      <td>-0.207027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perihelion_arg</th>\n",
       "      <td>-0.007669</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>0.031784</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>-0.020012</td>\n",
       "      <td>-0.044507</td>\n",
       "      <td>-0.053090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>-0.004517</td>\n",
       "      <td>-0.027294</td>\n",
       "      <td>0.067008</td>\n",
       "      <td>-0.003865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aphelion_dist</th>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>-0.256169</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021653</td>\n",
       "      <td>-0.012245</td>\n",
       "      <td>0.977630</td>\n",
       "      <td>0.292995</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.064609</td>\n",
       "      <td>-0.017011</td>\n",
       "      <td>-0.840166</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perihelion_time</th>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.115855</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>-0.058549</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>-0.004517</td>\n",
       "      <td>-0.064609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.038113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_anomaly</th>\n",
       "      <td>-0.051685</td>\n",
       "      <td>-0.051685</td>\n",
       "      <td>-0.049401</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015743</td>\n",
       "      <td>0.029477</td>\n",
       "      <td>-0.025304</td>\n",
       "      <td>-0.047114</td>\n",
       "      <td>-0.027294</td>\n",
       "      <td>-0.017011</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.054164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_motion</th>\n",
       "      <td>-0.020719</td>\n",
       "      <td>-0.020719</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>-0.859462</td>\n",
       "      <td>-0.601118</td>\n",
       "      <td>0.067008</td>\n",
       "      <td>-0.840166</td>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hazardous</th>\n",
       "      <td>-0.269028</td>\n",
       "      <td>-0.269028</td>\n",
       "      <td>-0.325522</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.017536</td>\n",
       "      <td>-0.011168</td>\n",
       "      <td>-0.207027</td>\n",
       "      <td>-0.003865</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.038113</td>\n",
       "      <td>0.054164</td>\n",
       "      <td>0.013028</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              neo_reference_id      name  absolute_magnitude  \\\n",
       "neo_reference_id                      1.000000  1.000000            0.602381   \n",
       "name                                  1.000000  1.000000            0.602381   \n",
       "absolute_magnitude                    0.602381  0.602381            1.000000   \n",
       "est_dia_in_km(min)                   -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_km(max)                   -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_m(min)                    -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_m(max)                    -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_miles(min)                -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_miles(max)                -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_feet(min)                 -0.499821 -0.499821           -0.613482   \n",
       "est_dia_in_feet(max)                 -0.499821 -0.499821           -0.613482   \n",
       "epoch_date_close_approach             0.186513  0.186513            0.168621   \n",
       "relative_velocity_km_per_sec         -0.165032 -0.165032           -0.376853   \n",
       "relative_velocity_km_per_hr          -0.165032 -0.165032           -0.376853   \n",
       "miles_per_hour                       -0.165032 -0.165032           -0.376853   \n",
       "miss_dist.(astronomical)             -0.155782 -0.155782           -0.339117   \n",
       "miss_dist.(lunar)                    -0.155782 -0.155782           -0.339117   \n",
       "miss_dist.(kilometers)               -0.155782 -0.155782           -0.339117   \n",
       "miss_dist.(miles)                    -0.155782 -0.155782           -0.339117   \n",
       "orbit_id                             -0.651200 -0.651200           -0.575668   \n",
       "orbit_uncertainity                    0.611205  0.611205            0.677764   \n",
       "minimum_orbit_intersection           -0.158673 -0.158673           -0.488235   \n",
       "jupiter_tisserand_invariant          -0.004854 -0.004854            0.238702   \n",
       "epoch_osculation                      0.006023  0.006023           -0.116087   \n",
       "eccentricity                         -0.125071 -0.125071           -0.361359   \n",
       "semi_major_axis                       0.035865  0.035865           -0.212437   \n",
       "inclination                          -0.175461 -0.175461           -0.459632   \n",
       "asc_node_longitude                   -0.026381 -0.026381           -0.011470   \n",
       "orbital_period                        0.040058  0.040058           -0.206774   \n",
       "perihelion_distance                   0.130486  0.130486            0.086966   \n",
       "perihelion_arg                       -0.007669 -0.007669            0.031784   \n",
       "aphelion_dist                         0.006318  0.006318           -0.256169   \n",
       "perihelion_time                       0.003031  0.003031           -0.115855   \n",
       "mean_anomaly                         -0.051685 -0.051685           -0.049401   \n",
       "mean_motion                          -0.020719 -0.020719            0.195652   \n",
       "hazardous                            -0.269028 -0.269028           -0.325522   \n",
       "\n",
       "                              est_dia_in_km(min)  est_dia_in_km(max)  \\\n",
       "neo_reference_id                       -0.499821           -0.499821   \n",
       "name                                   -0.499821           -0.499821   \n",
       "absolute_magnitude                     -0.613482           -0.613482   \n",
       "est_dia_in_km(min)                      1.000000            1.000000   \n",
       "est_dia_in_km(max)                      1.000000            1.000000   \n",
       "est_dia_in_m(min)                       1.000000            1.000000   \n",
       "est_dia_in_m(max)                       1.000000            1.000000   \n",
       "est_dia_in_miles(min)                   1.000000            1.000000   \n",
       "est_dia_in_miles(max)                   1.000000            1.000000   \n",
       "est_dia_in_feet(min)                    1.000000            1.000000   \n",
       "est_dia_in_feet(max)                    1.000000            1.000000   \n",
       "epoch_date_close_approach              -0.094121           -0.094121   \n",
       "relative_velocity_km_per_sec            0.242141            0.242141   \n",
       "relative_velocity_km_per_hr             0.242141            0.242141   \n",
       "miles_per_hour                          0.242141            0.242141   \n",
       "miss_dist.(astronomical)                0.188027            0.188027   \n",
       "miss_dist.(lunar)                       0.188027            0.188027   \n",
       "miss_dist.(kilometers)                  0.188027            0.188027   \n",
       "miss_dist.(miles)                       0.188027            0.188027   \n",
       "orbit_id                                0.724089            0.724089   \n",
       "orbit_uncertainity                     -0.399488           -0.399488   \n",
       "minimum_orbit_intersection              0.257904            0.257904   \n",
       "jupiter_tisserand_invariant            -0.133582           -0.133582   \n",
       "epoch_osculation                        0.061582            0.061582   \n",
       "eccentricity                            0.216623            0.216623   \n",
       "semi_major_axis                         0.121224            0.121224   \n",
       "inclination                             0.259450            0.259450   \n",
       "asc_node_longitude                      0.036558            0.036558   \n",
       "orbital_period                          0.118314            0.118314   \n",
       "perihelion_distance                    -0.071866           -0.071866   \n",
       "perihelion_arg                         -0.019577           -0.019577   \n",
       "aphelion_dist                           0.151836            0.151836   \n",
       "perihelion_time                         0.062167            0.062167   \n",
       "mean_anomaly                            0.031455            0.031455   \n",
       "mean_motion                            -0.104350           -0.104350   \n",
       "hazardous                               0.132424            0.132424   \n",
       "\n",
       "                              est_dia_in_m(min)  est_dia_in_m(max)  \\\n",
       "neo_reference_id                      -0.499821          -0.499821   \n",
       "name                                  -0.499821          -0.499821   \n",
       "absolute_magnitude                    -0.613482          -0.613482   \n",
       "est_dia_in_km(min)                     1.000000           1.000000   \n",
       "est_dia_in_km(max)                     1.000000           1.000000   \n",
       "est_dia_in_m(min)                      1.000000           1.000000   \n",
       "est_dia_in_m(max)                      1.000000           1.000000   \n",
       "est_dia_in_miles(min)                  1.000000           1.000000   \n",
       "est_dia_in_miles(max)                  1.000000           1.000000   \n",
       "est_dia_in_feet(min)                   1.000000           1.000000   \n",
       "est_dia_in_feet(max)                   1.000000           1.000000   \n",
       "epoch_date_close_approach             -0.094121          -0.094121   \n",
       "relative_velocity_km_per_sec           0.242141           0.242141   \n",
       "relative_velocity_km_per_hr            0.242141           0.242141   \n",
       "miles_per_hour                         0.242141           0.242141   \n",
       "miss_dist.(astronomical)               0.188027           0.188027   \n",
       "miss_dist.(lunar)                      0.188027           0.188027   \n",
       "miss_dist.(kilometers)                 0.188027           0.188027   \n",
       "miss_dist.(miles)                      0.188027           0.188027   \n",
       "orbit_id                               0.724089           0.724089   \n",
       "orbit_uncertainity                    -0.399488          -0.399488   \n",
       "minimum_orbit_intersection             0.257904           0.257904   \n",
       "jupiter_tisserand_invariant           -0.133582          -0.133582   \n",
       "epoch_osculation                       0.061582           0.061582   \n",
       "eccentricity                           0.216623           0.216623   \n",
       "semi_major_axis                        0.121224           0.121224   \n",
       "inclination                            0.259450           0.259450   \n",
       "asc_node_longitude                     0.036558           0.036558   \n",
       "orbital_period                         0.118314           0.118314   \n",
       "perihelion_distance                   -0.071866          -0.071866   \n",
       "perihelion_arg                        -0.019577          -0.019577   \n",
       "aphelion_dist                          0.151836           0.151836   \n",
       "perihelion_time                        0.062167           0.062167   \n",
       "mean_anomaly                           0.031455           0.031455   \n",
       "mean_motion                           -0.104350          -0.104350   \n",
       "hazardous                              0.132424           0.132424   \n",
       "\n",
       "                              est_dia_in_miles(min)  est_dia_in_miles(max)  \\\n",
       "neo_reference_id                          -0.499821              -0.499821   \n",
       "name                                      -0.499821              -0.499821   \n",
       "absolute_magnitude                        -0.613482              -0.613482   \n",
       "est_dia_in_km(min)                         1.000000               1.000000   \n",
       "est_dia_in_km(max)                         1.000000               1.000000   \n",
       "est_dia_in_m(min)                          1.000000               1.000000   \n",
       "est_dia_in_m(max)                          1.000000               1.000000   \n",
       "est_dia_in_miles(min)                      1.000000               1.000000   \n",
       "est_dia_in_miles(max)                      1.000000               1.000000   \n",
       "est_dia_in_feet(min)                       1.000000               1.000000   \n",
       "est_dia_in_feet(max)                       1.000000               1.000000   \n",
       "epoch_date_close_approach                 -0.094121              -0.094121   \n",
       "relative_velocity_km_per_sec               0.242141               0.242141   \n",
       "relative_velocity_km_per_hr                0.242141               0.242141   \n",
       "miles_per_hour                             0.242141               0.242141   \n",
       "miss_dist.(astronomical)                   0.188027               0.188027   \n",
       "miss_dist.(lunar)                          0.188027               0.188027   \n",
       "miss_dist.(kilometers)                     0.188027               0.188027   \n",
       "miss_dist.(miles)                          0.188027               0.188027   \n",
       "orbit_id                                   0.724089               0.724089   \n",
       "orbit_uncertainity                        -0.399488              -0.399488   \n",
       "minimum_orbit_intersection                 0.257904               0.257904   \n",
       "jupiter_tisserand_invariant               -0.133582              -0.133582   \n",
       "epoch_osculation                           0.061582               0.061582   \n",
       "eccentricity                               0.216623               0.216623   \n",
       "semi_major_axis                            0.121224               0.121224   \n",
       "inclination                                0.259450               0.259450   \n",
       "asc_node_longitude                         0.036558               0.036558   \n",
       "orbital_period                             0.118314               0.118314   \n",
       "perihelion_distance                       -0.071866              -0.071866   \n",
       "perihelion_arg                            -0.019577              -0.019577   \n",
       "aphelion_dist                              0.151836               0.151836   \n",
       "perihelion_time                            0.062167               0.062167   \n",
       "mean_anomaly                               0.031455               0.031455   \n",
       "mean_motion                               -0.104350              -0.104350   \n",
       "hazardous                                  0.132424               0.132424   \n",
       "\n",
       "                              est_dia_in_feet(min)  ...  inclination  \\\n",
       "neo_reference_id                         -0.499821  ...    -0.175461   \n",
       "name                                     -0.499821  ...    -0.175461   \n",
       "absolute_magnitude                       -0.613482  ...    -0.459632   \n",
       "est_dia_in_km(min)                        1.000000  ...     0.259450   \n",
       "est_dia_in_km(max)                        1.000000  ...     0.259450   \n",
       "est_dia_in_m(min)                         1.000000  ...     0.259450   \n",
       "est_dia_in_m(max)                         1.000000  ...     0.259450   \n",
       "est_dia_in_miles(min)                     1.000000  ...     0.259450   \n",
       "est_dia_in_miles(max)                     1.000000  ...     0.259450   \n",
       "est_dia_in_feet(min)                      1.000000  ...     0.259450   \n",
       "est_dia_in_feet(max)                      1.000000  ...     0.259450   \n",
       "epoch_date_close_approach                -0.094121  ...    -0.067337   \n",
       "relative_velocity_km_per_sec              0.242141  ...     0.514657   \n",
       "relative_velocity_km_per_hr               0.242141  ...     0.514657   \n",
       "miles_per_hour                            0.242141  ...     0.514657   \n",
       "miss_dist.(astronomical)                  0.188027  ...     0.255828   \n",
       "miss_dist.(lunar)                         0.188027  ...     0.255828   \n",
       "miss_dist.(kilometers)                    0.188027  ...     0.255828   \n",
       "miss_dist.(miles)                         0.188027  ...     0.255828   \n",
       "orbit_id                                  0.724089  ...     0.112125   \n",
       "orbit_uncertainity                       -0.399488  ...    -0.228806   \n",
       "minimum_orbit_intersection                0.257904  ...     0.439867   \n",
       "jupiter_tisserand_invariant              -0.133582  ...    -0.037366   \n",
       "epoch_osculation                          0.061582  ...     0.015879   \n",
       "eccentricity                              0.216623  ...     0.039018   \n",
       "semi_major_axis                           0.121224  ...    -0.030325   \n",
       "inclination                               0.259450  ...     1.000000   \n",
       "asc_node_longitude                        0.036558  ...    -0.029160   \n",
       "orbital_period                            0.118314  ...    -0.032227   \n",
       "perihelion_distance                      -0.071866  ...    -0.046215   \n",
       "perihelion_arg                           -0.019577  ...     0.003301   \n",
       "aphelion_dist                             0.151836  ...    -0.021653   \n",
       "perihelion_time                           0.062167  ...     0.013727   \n",
       "mean_anomaly                              0.031455  ...     0.015743   \n",
       "mean_motion                              -0.104350  ...     0.013188   \n",
       "hazardous                                 0.132424  ...     0.009607   \n",
       "\n",
       "                              asc_node_longitude  orbital_period  \\\n",
       "neo_reference_id                       -0.026381        0.040058   \n",
       "name                                   -0.026381        0.040058   \n",
       "absolute_magnitude                     -0.011470       -0.206774   \n",
       "est_dia_in_km(min)                      0.036558        0.118314   \n",
       "est_dia_in_km(max)                      0.036558        0.118314   \n",
       "est_dia_in_m(min)                       0.036558        0.118314   \n",
       "est_dia_in_m(max)                       0.036558        0.118314   \n",
       "est_dia_in_miles(min)                   0.036558        0.118314   \n",
       "est_dia_in_miles(max)                   0.036558        0.118314   \n",
       "est_dia_in_feet(min)                    0.036558        0.118314   \n",
       "est_dia_in_feet(max)                    0.036558        0.118314   \n",
       "epoch_date_close_approach              -0.019341        0.130175   \n",
       "relative_velocity_km_per_sec           -0.021301        0.017961   \n",
       "relative_velocity_km_per_hr            -0.021301        0.017961   \n",
       "miles_per_hour                         -0.021301        0.017961   \n",
       "miss_dist.(astronomical)               -0.023033       -0.109888   \n",
       "miss_dist.(lunar)                      -0.023033       -0.109888   \n",
       "miss_dist.(kilometers)                 -0.023033       -0.109888   \n",
       "miss_dist.(miles)                      -0.023033       -0.109888   \n",
       "orbit_id                                0.047959        0.002705   \n",
       "orbit_uncertainity                     -0.009618        0.047535   \n",
       "minimum_orbit_intersection             -0.008963        0.279885   \n",
       "jupiter_tisserand_invariant             0.018413       -0.893517   \n",
       "epoch_osculation                        0.017782       -0.056653   \n",
       "eccentricity                           -0.015413        0.548521   \n",
       "semi_major_axis                        -0.011073        0.995248   \n",
       "inclination                            -0.029160       -0.032227   \n",
       "asc_node_longitude                      1.000000       -0.009580   \n",
       "orbital_period                         -0.009580        1.000000   \n",
       "perihelion_distance                     0.000182        0.467209   \n",
       "perihelion_arg                         -0.020012       -0.044507   \n",
       "aphelion_dist                          -0.012245        0.977630   \n",
       "perihelion_time                         0.020059       -0.058549   \n",
       "mean_anomaly                            0.029477       -0.025304   \n",
       "mean_motion                             0.017870       -0.859462   \n",
       "hazardous                               0.017536       -0.011168   \n",
       "\n",
       "                              perihelion_distance  perihelion_arg  \\\n",
       "neo_reference_id                         0.130486       -0.007669   \n",
       "name                                     0.130486       -0.007669   \n",
       "absolute_magnitude                       0.086966        0.031784   \n",
       "est_dia_in_km(min)                      -0.071866       -0.019577   \n",
       "est_dia_in_km(max)                      -0.071866       -0.019577   \n",
       "est_dia_in_m(min)                       -0.071866       -0.019577   \n",
       "est_dia_in_m(max)                       -0.071866       -0.019577   \n",
       "est_dia_in_miles(min)                   -0.071866       -0.019577   \n",
       "est_dia_in_miles(max)                   -0.071866       -0.019577   \n",
       "est_dia_in_feet(min)                    -0.071866       -0.019577   \n",
       "est_dia_in_feet(max)                    -0.071866       -0.019577   \n",
       "epoch_date_close_approach                0.131854        0.001834   \n",
       "relative_velocity_km_per_sec            -0.506978       -0.002913   \n",
       "relative_velocity_km_per_hr             -0.506978       -0.002913   \n",
       "miles_per_hour                          -0.506978       -0.002913   \n",
       "miss_dist.(astronomical)                -0.079937       -0.021483   \n",
       "miss_dist.(lunar)                       -0.079937       -0.021483   \n",
       "miss_dist.(kilometers)                  -0.079937       -0.021483   \n",
       "miss_dist.(miles)                       -0.079937       -0.021483   \n",
       "orbit_id                                -0.063884        0.001338   \n",
       "orbit_uncertainity                       0.107946        0.016307   \n",
       "minimum_orbit_intersection               0.299991       -0.029130   \n",
       "jupiter_tisserand_invariant             -0.537884        0.064496   \n",
       "epoch_osculation                         0.000093       -0.006703   \n",
       "eccentricity                            -0.412612       -0.003210   \n",
       "semi_major_axis                          0.496847       -0.048999   \n",
       "inclination                             -0.046215        0.003301   \n",
       "asc_node_longitude                       0.000182       -0.020012   \n",
       "orbital_period                           0.467209       -0.044507   \n",
       "perihelion_distance                      1.000000       -0.053090   \n",
       "perihelion_arg                          -0.053090        1.000000   \n",
       "aphelion_dist                            0.292995       -0.040477   \n",
       "perihelion_time                         -0.002854       -0.004517   \n",
       "mean_anomaly                            -0.047114       -0.027294   \n",
       "mean_motion                             -0.601118        0.067008   \n",
       "hazardous                               -0.207027       -0.003865   \n",
       "\n",
       "                              aphelion_dist  perihelion_time  mean_anomaly  \\\n",
       "neo_reference_id                   0.006318         0.003031     -0.051685   \n",
       "name                               0.006318         0.003031     -0.051685   \n",
       "absolute_magnitude                -0.256169        -0.115855     -0.049401   \n",
       "est_dia_in_km(min)                 0.151836         0.062167      0.031455   \n",
       "est_dia_in_km(max)                 0.151836         0.062167      0.031455   \n",
       "est_dia_in_m(min)                  0.151836         0.062167      0.031455   \n",
       "est_dia_in_m(max)                  0.151836         0.062167      0.031455   \n",
       "est_dia_in_miles(min)              0.151836         0.062167      0.031455   \n",
       "est_dia_in_miles(max)              0.151836         0.062167      0.031455   \n",
       "est_dia_in_feet(min)               0.151836         0.062167      0.031455   \n",
       "est_dia_in_feet(max)               0.151836         0.062167      0.031455   \n",
       "epoch_date_close_approach          0.114616        -0.015533     -0.026182   \n",
       "relative_velocity_km_per_sec       0.144782         0.020006      0.017685   \n",
       "relative_velocity_km_per_hr        0.144782         0.020006      0.017685   \n",
       "miles_per_hour                     0.144782         0.020006      0.017685   \n",
       "miss_dist.(astronomical)          -0.103231         0.189409     -0.010157   \n",
       "miss_dist.(lunar)                 -0.103231         0.189409     -0.010157   \n",
       "miss_dist.(kilometers)            -0.103231         0.189409     -0.010157   \n",
       "miss_dist.(miles)                 -0.103231         0.189409     -0.010157   \n",
       "orbit_id                           0.022269         0.037901      0.048985   \n",
       "orbit_uncertainity                 0.020908        -0.325784     -0.059396   \n",
       "minimum_orbit_intersection         0.242023         0.080592     -0.013336   \n",
       "jupiter_tisserand_invariant       -0.887879         0.051994      0.030972   \n",
       "epoch_osculation                  -0.063625         0.977613      0.036881   \n",
       "eccentricity                       0.701294        -0.064366      0.026161   \n",
       "semi_major_axis                    0.975326        -0.059303     -0.026319   \n",
       "inclination                       -0.021653         0.013727      0.015743   \n",
       "asc_node_longitude                -0.012245         0.020059      0.029477   \n",
       "orbital_period                     0.977630        -0.058549     -0.025304   \n",
       "perihelion_distance                0.292995        -0.002854     -0.047114   \n",
       "perihelion_arg                    -0.040477        -0.004517     -0.027294   \n",
       "aphelion_dist                      1.000000        -0.064609     -0.017011   \n",
       "perihelion_time                   -0.064609         1.000000      0.125563   \n",
       "mean_anomaly                      -0.017011         0.125563      1.000000   \n",
       "mean_motion                       -0.840166         0.047035      0.035849   \n",
       "hazardous                          0.040800         0.038113      0.054164   \n",
       "\n",
       "                              mean_motion  hazardous  \n",
       "neo_reference_id                -0.020719  -0.269028  \n",
       "name                            -0.020719  -0.269028  \n",
       "absolute_magnitude               0.195652  -0.325522  \n",
       "est_dia_in_km(min)              -0.104350   0.132424  \n",
       "est_dia_in_km(max)              -0.104350   0.132424  \n",
       "est_dia_in_m(min)               -0.104350   0.132424  \n",
       "est_dia_in_m(max)               -0.104350   0.132424  \n",
       "est_dia_in_miles(min)           -0.104350   0.132424  \n",
       "est_dia_in_miles(max)           -0.104350   0.132424  \n",
       "est_dia_in_feet(min)            -0.104350   0.132424  \n",
       "est_dia_in_feet(max)            -0.104350   0.132424  \n",
       "epoch_date_close_approach       -0.137663  -0.079020  \n",
       "relative_velocity_km_per_sec     0.022452   0.191970  \n",
       "relative_velocity_km_per_hr      0.022452   0.191970  \n",
       "miles_per_hour                   0.022452   0.191970  \n",
       "miss_dist.(astronomical)         0.104642   0.032407  \n",
       "miss_dist.(lunar)                0.104642   0.032407  \n",
       "miss_dist.(kilometers)           0.104642   0.032407  \n",
       "miss_dist.(miles)                0.104642   0.032407  \n",
       "orbit_id                        -0.008898   0.247369  \n",
       "orbit_uncertainity              -0.023776  -0.328721  \n",
       "minimum_orbit_intersection      -0.290538  -0.288949  \n",
       "jupiter_tisserand_invariant      0.992680  -0.003404  \n",
       "epoch_osculation                 0.045812   0.040940  \n",
       "eccentricity                    -0.394860   0.183269  \n",
       "semi_major_axis                 -0.901396  -0.010770  \n",
       "inclination                      0.013188   0.009607  \n",
       "asc_node_longitude               0.017870   0.017536  \n",
       "orbital_period                  -0.859462  -0.011168  \n",
       "perihelion_distance             -0.601118  -0.207027  \n",
       "perihelion_arg                   0.067008  -0.003865  \n",
       "aphelion_dist                   -0.840166   0.040800  \n",
       "perihelion_time                  0.047035   0.038113  \n",
       "mean_anomaly                     0.035849   0.054164  \n",
       "mean_motion                      1.000000   0.013028  \n",
       "hazardous                        0.013028   1.000000  \n",
       "\n",
       "[36 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neo_reference_id                0\n",
       "name                            0\n",
       "absolute_magnitude              0\n",
       "est_dia_in_km(min)              0\n",
       "est_dia_in_km(max)              0\n",
       "est_dia_in_m(min)               0\n",
       "est_dia_in_m(max)               0\n",
       "est_dia_in_miles(min)           0\n",
       "est_dia_in_miles(max)           0\n",
       "est_dia_in_feet(min)            0\n",
       "est_dia_in_feet(max)            0\n",
       "close_approach_date             0\n",
       "epoch_date_close_approach       0\n",
       "relative_velocity_km_per_sec    0\n",
       "relative_velocity_km_per_hr     0\n",
       "miles_per_hour                  0\n",
       "miss_dist.(astronomical)        0\n",
       "miss_dist.(lunar)               0\n",
       "miss_dist.(kilometers)          0\n",
       "miss_dist.(miles)               0\n",
       "orbiting_body                   0\n",
       "orbit_id                        0\n",
       "orbit_determination_date        0\n",
       "orbit_uncertainity              0\n",
       "minimum_orbit_intersection      0\n",
       "jupiter_tisserand_invariant     0\n",
       "epoch_osculation                0\n",
       "eccentricity                    0\n",
       "semi_major_axis                 0\n",
       "inclination                     0\n",
       "asc_node_longitude              0\n",
       "orbital_period                  0\n",
       "perihelion_distance             0\n",
       "perihelion_arg                  0\n",
       "aphelion_dist                   0\n",
       "perihelion_time                 0\n",
       "mean_anomaly                    0\n",
       "mean_motion                     0\n",
       "equinox                         0\n",
       "hazardous                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2017-04-06 08:36:37', '2017-04-06 08:32:49',\n",
       "       '2017-04-06 09:20:19', ..., '2017-04-06 08:23:43',\n",
       "       '2017-04-06 08:23:42', '2017-04-29 06:18:33'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['orbit_determination_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['J2000'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['equinox'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Earth'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['orbiting_body'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['close_approach_date']=df['close_approach_date']rename.replace({'-',''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_integer(dt_time):\n",
    "#     return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1995-01-01\n",
       "1       1995-01-01\n",
       "2       1995-01-08\n",
       "3       1995-01-15\n",
       "4       1995-01-15\n",
       "           ...    \n",
       "4682    2016-09-08\n",
       "4683    2016-09-08\n",
       "4684    2016-09-08\n",
       "4685    2016-09-08\n",
       "4686    2016-09-08\n",
       "Name: close_approach_date, Length: 4687, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['close_approach_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4687, 40)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['neo_reference_id','name','orbiting_body','equinox','orbit_determination_date','close_approach_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4687, 34)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hazardous']=pd.get_dummies(df['hazardous'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "4682    0\n",
       "4683    0\n",
       "4684    0\n",
       "4685    0\n",
       "4686    0\n",
       "Name: hazardous, Length: 4687, dtype: uint8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hazardous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_magnitude</th>\n",
       "      <th>est_dia_in_km(min)</th>\n",
       "      <th>est_dia_in_km(max)</th>\n",
       "      <th>est_dia_in_m(min)</th>\n",
       "      <th>est_dia_in_m(max)</th>\n",
       "      <th>est_dia_in_miles(min)</th>\n",
       "      <th>est_dia_in_miles(max)</th>\n",
       "      <th>est_dia_in_feet(min)</th>\n",
       "      <th>est_dia_in_feet(max)</th>\n",
       "      <th>epoch_date_close_approach</th>\n",
       "      <th>...</th>\n",
       "      <th>inclination</th>\n",
       "      <th>asc_node_longitude</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>perihelion_distance</th>\n",
       "      <th>perihelion_arg</th>\n",
       "      <th>aphelion_dist</th>\n",
       "      <th>perihelion_time</th>\n",
       "      <th>mean_anomaly</th>\n",
       "      <th>mean_motion</th>\n",
       "      <th>hazardous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>933.308089</td>\n",
       "      <td>788947200000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.025981</td>\n",
       "      <td>314.373913</td>\n",
       "      <td>609.599786</td>\n",
       "      <td>0.808259</td>\n",
       "      <td>57.257470</td>\n",
       "      <td>2.005764</td>\n",
       "      <td>2.458162e+06</td>\n",
       "      <td>264.837533</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.300</td>\n",
       "      <td>0.146068</td>\n",
       "      <td>0.326618</td>\n",
       "      <td>146.067964</td>\n",
       "      <td>326.617897</td>\n",
       "      <td>0.090762</td>\n",
       "      <td>0.202951</td>\n",
       "      <td>479.225620</td>\n",
       "      <td>1071.581063</td>\n",
       "      <td>788947200000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.412996</td>\n",
       "      <td>136.717242</td>\n",
       "      <td>425.869294</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>313.091975</td>\n",
       "      <td>1.497352</td>\n",
       "      <td>2.457795e+06</td>\n",
       "      <td>173.741112</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.300</td>\n",
       "      <td>0.231502</td>\n",
       "      <td>0.517654</td>\n",
       "      <td>231.502122</td>\n",
       "      <td>517.654482</td>\n",
       "      <td>0.143849</td>\n",
       "      <td>0.321655</td>\n",
       "      <td>759.521423</td>\n",
       "      <td>1698.341531</td>\n",
       "      <td>789552000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.237961</td>\n",
       "      <td>259.475979</td>\n",
       "      <td>643.580228</td>\n",
       "      <td>0.950791</td>\n",
       "      <td>248.415038</td>\n",
       "      <td>1.966857</td>\n",
       "      <td>2.458120e+06</td>\n",
       "      <td>292.893654</td>\n",
       "      <td>0.559371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.400</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>8.801465</td>\n",
       "      <td>19.680675</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>28.876199</td>\n",
       "      <td>64.569144</td>\n",
       "      <td>790156800000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.905894</td>\n",
       "      <td>57.173266</td>\n",
       "      <td>514.082140</td>\n",
       "      <td>0.983902</td>\n",
       "      <td>18.707701</td>\n",
       "      <td>1.527904</td>\n",
       "      <td>2.457902e+06</td>\n",
       "      <td>68.741007</td>\n",
       "      <td>0.700277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>933.308089</td>\n",
       "      <td>790156800000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.793382</td>\n",
       "      <td>84.629307</td>\n",
       "      <td>495.597821</td>\n",
       "      <td>0.967687</td>\n",
       "      <td>158.263596</td>\n",
       "      <td>1.483543</td>\n",
       "      <td>2.457814e+06</td>\n",
       "      <td>135.142133</td>\n",
       "      <td>0.726395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>23.900</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>0.098637</td>\n",
       "      <td>44.111820</td>\n",
       "      <td>98.637028</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>144.723824</td>\n",
       "      <td>323.612307</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.880491</td>\n",
       "      <td>164.183305</td>\n",
       "      <td>457.179984</td>\n",
       "      <td>0.741558</td>\n",
       "      <td>276.395697</td>\n",
       "      <td>1.581299</td>\n",
       "      <td>2.457708e+06</td>\n",
       "      <td>304.306025</td>\n",
       "      <td>0.787436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>28.200</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>6.089126</td>\n",
       "      <td>13.615700</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>19.977449</td>\n",
       "      <td>44.670934</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.360249</td>\n",
       "      <td>345.225230</td>\n",
       "      <td>407.185767</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>42.111064</td>\n",
       "      <td>1.153835</td>\n",
       "      <td>2.458088e+06</td>\n",
       "      <td>282.978786</td>\n",
       "      <td>0.884117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>22.700</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.171412</td>\n",
       "      <td>76.657557</td>\n",
       "      <td>171.411509</td>\n",
       "      <td>0.047633</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>251.501180</td>\n",
       "      <td>562.373736</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.405467</td>\n",
       "      <td>37.026468</td>\n",
       "      <td>690.054279</td>\n",
       "      <td>0.965760</td>\n",
       "      <td>274.692712</td>\n",
       "      <td>2.090708</td>\n",
       "      <td>2.458300e+06</td>\n",
       "      <td>203.501147</td>\n",
       "      <td>0.521698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>21.800</td>\n",
       "      <td>0.116026</td>\n",
       "      <td>0.259442</td>\n",
       "      <td>116.025908</td>\n",
       "      <td>259.441818</td>\n",
       "      <td>0.072095</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>380.662441</td>\n",
       "      <td>851.187094</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080244</td>\n",
       "      <td>163.802910</td>\n",
       "      <td>662.048343</td>\n",
       "      <td>1.185467</td>\n",
       "      <td>180.346090</td>\n",
       "      <td>1.787733</td>\n",
       "      <td>2.458288e+06</td>\n",
       "      <td>203.524965</td>\n",
       "      <td>0.543767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>19.109</td>\n",
       "      <td>0.400641</td>\n",
       "      <td>0.895860</td>\n",
       "      <td>400.640618</td>\n",
       "      <td>895.859655</td>\n",
       "      <td>0.248946</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>1314.437764</td>\n",
       "      <td>2939.172192</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>53.574923</td>\n",
       "      <td>187.642183</td>\n",
       "      <td>653.679098</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>222.436688</td>\n",
       "      <td>2.071980</td>\n",
       "      <td>2.458319e+06</td>\n",
       "      <td>184.820424</td>\n",
       "      <td>0.550729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4687 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolute_magnitude  est_dia_in_km(min)  est_dia_in_km(max)  \\\n",
       "0                 21.600            0.127220            0.284472   \n",
       "1                 21.300            0.146068            0.326618   \n",
       "2                 20.300            0.231502            0.517654   \n",
       "3                 27.400            0.008801            0.019681   \n",
       "4                 21.600            0.127220            0.284472   \n",
       "...                  ...                 ...                 ...   \n",
       "4682              23.900            0.044112            0.098637   \n",
       "4683              28.200            0.006089            0.013616   \n",
       "4684              22.700            0.076658            0.171412   \n",
       "4685              21.800            0.116026            0.259442   \n",
       "4686              19.109            0.400641            0.895860   \n",
       "\n",
       "      est_dia_in_m(min)  est_dia_in_m(max)  est_dia_in_miles(min)  \\\n",
       "0            127.219879         284.472297               0.079051   \n",
       "1            146.067964         326.617897               0.090762   \n",
       "2            231.502122         517.654482               0.143849   \n",
       "3              8.801465          19.680675               0.005469   \n",
       "4            127.219879         284.472297               0.079051   \n",
       "...                 ...                ...                    ...   \n",
       "4682          44.111820          98.637028               0.027410   \n",
       "4683           6.089126          13.615700               0.003784   \n",
       "4684          76.657557         171.411509               0.047633   \n",
       "4685         116.025908         259.441818               0.072095   \n",
       "4686         400.640618         895.859655               0.248946   \n",
       "\n",
       "      est_dia_in_miles(max)  est_dia_in_feet(min)  est_dia_in_feet(max)  \\\n",
       "0                  0.176763            417.388066            933.308089   \n",
       "1                  0.202951            479.225620           1071.581063   \n",
       "2                  0.321655            759.521423           1698.341531   \n",
       "3                  0.012229             28.876199             64.569144   \n",
       "4                  0.176763            417.388066            933.308089   \n",
       "...                     ...                   ...                   ...   \n",
       "4682               0.061290            144.723824            323.612307   \n",
       "4683               0.008460             19.977449             44.670934   \n",
       "4684               0.106510            251.501180            562.373736   \n",
       "4685               0.161210            380.662441            851.187094   \n",
       "4686               0.556661           1314.437764           2939.172192   \n",
       "\n",
       "      epoch_date_close_approach  ...  inclination  asc_node_longitude  \\\n",
       "0                  788947200000  ...     6.025981          314.373913   \n",
       "1                  788947200000  ...    28.412996          136.717242   \n",
       "2                  789552000000  ...     4.237961          259.475979   \n",
       "3                  790156800000  ...     7.905894           57.173266   \n",
       "4                  790156800000  ...    16.793382           84.629307   \n",
       "...                         ...  ...          ...                 ...   \n",
       "4682              1473318000000  ...    39.880491          164.183305   \n",
       "4683              1473318000000  ...     5.360249          345.225230   \n",
       "4684              1473318000000  ...     4.405467           37.026468   \n",
       "4685              1473318000000  ...    21.080244          163.802910   \n",
       "4686              1473318000000  ...    53.574923          187.642183   \n",
       "\n",
       "      orbital_period  perihelion_distance  perihelion_arg  aphelion_dist  \\\n",
       "0         609.599786             0.808259       57.257470       2.005764   \n",
       "1         425.869294             0.718200      313.091975       1.497352   \n",
       "2         643.580228             0.950791      248.415038       1.966857   \n",
       "3         514.082140             0.983902       18.707701       1.527904   \n",
       "4         495.597821             0.967687      158.263596       1.483543   \n",
       "...              ...                  ...             ...            ...   \n",
       "4682      457.179984             0.741558      276.395697       1.581299   \n",
       "4683      407.185767             0.996434       42.111064       1.153835   \n",
       "4684      690.054279             0.965760      274.692712       2.090708   \n",
       "4685      662.048343             1.185467      180.346090       1.787733   \n",
       "4686      653.679098             0.876110      222.436688       2.071980   \n",
       "\n",
       "      perihelion_time  mean_anomaly  mean_motion  hazardous  \n",
       "0        2.458162e+06    264.837533     0.590551          1  \n",
       "1        2.457795e+06    173.741112     0.845330          0  \n",
       "2        2.458120e+06    292.893654     0.559371          1  \n",
       "3        2.457902e+06     68.741007     0.700277          0  \n",
       "4        2.457814e+06    135.142133     0.726395          1  \n",
       "...               ...           ...          ...        ...  \n",
       "4682     2.457708e+06    304.306025     0.787436          0  \n",
       "4683     2.458088e+06    282.978786     0.884117          0  \n",
       "4684     2.458300e+06    203.501147     0.521698          0  \n",
       "4685     2.458288e+06    203.524965     0.543767          0  \n",
       "4686     2.458319e+06    184.820424     0.550729          0  \n",
       "\n",
       "[4687 rows x 34 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:,:-1]\n",
    "y=df.loc[:,\"hazardous\"]\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=101,stratify=y)\n",
    "#x_train,x_test,y_train,y_test=train_test_split(df.drop('hazardous',axis=1),df['hazardous'],test_size=0.25,random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_magnitude</th>\n",
       "      <th>est_dia_in_km(min)</th>\n",
       "      <th>est_dia_in_km(max)</th>\n",
       "      <th>est_dia_in_m(min)</th>\n",
       "      <th>est_dia_in_m(max)</th>\n",
       "      <th>est_dia_in_miles(min)</th>\n",
       "      <th>est_dia_in_miles(max)</th>\n",
       "      <th>est_dia_in_feet(min)</th>\n",
       "      <th>est_dia_in_feet(max)</th>\n",
       "      <th>epoch_date_close_approach</th>\n",
       "      <th>...</th>\n",
       "      <th>semi_major_axis</th>\n",
       "      <th>inclination</th>\n",
       "      <th>asc_node_longitude</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>perihelion_distance</th>\n",
       "      <th>perihelion_arg</th>\n",
       "      <th>aphelion_dist</th>\n",
       "      <th>perihelion_time</th>\n",
       "      <th>mean_anomaly</th>\n",
       "      <th>mean_motion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>933.308089</td>\n",
       "      <td>788947200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.407011</td>\n",
       "      <td>6.025981</td>\n",
       "      <td>314.373913</td>\n",
       "      <td>609.599786</td>\n",
       "      <td>0.808259</td>\n",
       "      <td>57.257470</td>\n",
       "      <td>2.005764</td>\n",
       "      <td>2.458162e+06</td>\n",
       "      <td>264.837533</td>\n",
       "      <td>0.590551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.300</td>\n",
       "      <td>0.146068</td>\n",
       "      <td>0.326618</td>\n",
       "      <td>146.067964</td>\n",
       "      <td>326.617897</td>\n",
       "      <td>0.090762</td>\n",
       "      <td>0.202951</td>\n",
       "      <td>479.225620</td>\n",
       "      <td>1071.581063</td>\n",
       "      <td>788947200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107776</td>\n",
       "      <td>28.412996</td>\n",
       "      <td>136.717242</td>\n",
       "      <td>425.869294</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>313.091975</td>\n",
       "      <td>1.497352</td>\n",
       "      <td>2.457795e+06</td>\n",
       "      <td>173.741112</td>\n",
       "      <td>0.845330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.300</td>\n",
       "      <td>0.231502</td>\n",
       "      <td>0.517654</td>\n",
       "      <td>231.502122</td>\n",
       "      <td>517.654482</td>\n",
       "      <td>0.143849</td>\n",
       "      <td>0.321655</td>\n",
       "      <td>759.521423</td>\n",
       "      <td>1698.341531</td>\n",
       "      <td>789552000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458824</td>\n",
       "      <td>4.237961</td>\n",
       "      <td>259.475979</td>\n",
       "      <td>643.580228</td>\n",
       "      <td>0.950791</td>\n",
       "      <td>248.415038</td>\n",
       "      <td>1.966857</td>\n",
       "      <td>2.458120e+06</td>\n",
       "      <td>292.893654</td>\n",
       "      <td>0.559371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.400</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>8.801465</td>\n",
       "      <td>19.680675</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>28.876199</td>\n",
       "      <td>64.569144</td>\n",
       "      <td>790156800000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.255903</td>\n",
       "      <td>7.905894</td>\n",
       "      <td>57.173266</td>\n",
       "      <td>514.082140</td>\n",
       "      <td>0.983902</td>\n",
       "      <td>18.707701</td>\n",
       "      <td>1.527904</td>\n",
       "      <td>2.457902e+06</td>\n",
       "      <td>68.741007</td>\n",
       "      <td>0.700277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.600</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>127.219879</td>\n",
       "      <td>284.472297</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>417.388066</td>\n",
       "      <td>933.308089</td>\n",
       "      <td>790156800000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.225615</td>\n",
       "      <td>16.793382</td>\n",
       "      <td>84.629307</td>\n",
       "      <td>495.597821</td>\n",
       "      <td>0.967687</td>\n",
       "      <td>158.263596</td>\n",
       "      <td>1.483543</td>\n",
       "      <td>2.457814e+06</td>\n",
       "      <td>135.142133</td>\n",
       "      <td>0.726395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>23.900</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>0.098637</td>\n",
       "      <td>44.111820</td>\n",
       "      <td>98.637028</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>144.723824</td>\n",
       "      <td>323.612307</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.161429</td>\n",
       "      <td>39.880491</td>\n",
       "      <td>164.183305</td>\n",
       "      <td>457.179984</td>\n",
       "      <td>0.741558</td>\n",
       "      <td>276.395697</td>\n",
       "      <td>1.581299</td>\n",
       "      <td>2.457708e+06</td>\n",
       "      <td>304.306025</td>\n",
       "      <td>0.787436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>28.200</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>6.089126</td>\n",
       "      <td>13.615700</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>19.977449</td>\n",
       "      <td>44.670934</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.075134</td>\n",
       "      <td>5.360249</td>\n",
       "      <td>345.225230</td>\n",
       "      <td>407.185767</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>42.111064</td>\n",
       "      <td>1.153835</td>\n",
       "      <td>2.458088e+06</td>\n",
       "      <td>282.978786</td>\n",
       "      <td>0.884117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>22.700</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.171412</td>\n",
       "      <td>76.657557</td>\n",
       "      <td>171.411509</td>\n",
       "      <td>0.047633</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>251.501180</td>\n",
       "      <td>562.373736</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.528234</td>\n",
       "      <td>4.405467</td>\n",
       "      <td>37.026468</td>\n",
       "      <td>690.054279</td>\n",
       "      <td>0.965760</td>\n",
       "      <td>274.692712</td>\n",
       "      <td>2.090708</td>\n",
       "      <td>2.458300e+06</td>\n",
       "      <td>203.501147</td>\n",
       "      <td>0.521698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>21.800</td>\n",
       "      <td>0.116026</td>\n",
       "      <td>0.259442</td>\n",
       "      <td>116.025908</td>\n",
       "      <td>259.441818</td>\n",
       "      <td>0.072095</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>380.662441</td>\n",
       "      <td>851.187094</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.486600</td>\n",
       "      <td>21.080244</td>\n",
       "      <td>163.802910</td>\n",
       "      <td>662.048343</td>\n",
       "      <td>1.185467</td>\n",
       "      <td>180.346090</td>\n",
       "      <td>1.787733</td>\n",
       "      <td>2.458288e+06</td>\n",
       "      <td>203.524965</td>\n",
       "      <td>0.543767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>19.109</td>\n",
       "      <td>0.400641</td>\n",
       "      <td>0.895860</td>\n",
       "      <td>400.640618</td>\n",
       "      <td>895.859655</td>\n",
       "      <td>0.248946</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>1314.437764</td>\n",
       "      <td>2939.172192</td>\n",
       "      <td>1473318000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.474045</td>\n",
       "      <td>53.574923</td>\n",
       "      <td>187.642183</td>\n",
       "      <td>653.679098</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>222.436688</td>\n",
       "      <td>2.071980</td>\n",
       "      <td>2.458319e+06</td>\n",
       "      <td>184.820424</td>\n",
       "      <td>0.550729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4687 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolute_magnitude  est_dia_in_km(min)  est_dia_in_km(max)  \\\n",
       "0                 21.600            0.127220            0.284472   \n",
       "1                 21.300            0.146068            0.326618   \n",
       "2                 20.300            0.231502            0.517654   \n",
       "3                 27.400            0.008801            0.019681   \n",
       "4                 21.600            0.127220            0.284472   \n",
       "...                  ...                 ...                 ...   \n",
       "4682              23.900            0.044112            0.098637   \n",
       "4683              28.200            0.006089            0.013616   \n",
       "4684              22.700            0.076658            0.171412   \n",
       "4685              21.800            0.116026            0.259442   \n",
       "4686              19.109            0.400641            0.895860   \n",
       "\n",
       "      est_dia_in_m(min)  est_dia_in_m(max)  est_dia_in_miles(min)  \\\n",
       "0            127.219879         284.472297               0.079051   \n",
       "1            146.067964         326.617897               0.090762   \n",
       "2            231.502122         517.654482               0.143849   \n",
       "3              8.801465          19.680675               0.005469   \n",
       "4            127.219879         284.472297               0.079051   \n",
       "...                 ...                ...                    ...   \n",
       "4682          44.111820          98.637028               0.027410   \n",
       "4683           6.089126          13.615700               0.003784   \n",
       "4684          76.657557         171.411509               0.047633   \n",
       "4685         116.025908         259.441818               0.072095   \n",
       "4686         400.640618         895.859655               0.248946   \n",
       "\n",
       "      est_dia_in_miles(max)  est_dia_in_feet(min)  est_dia_in_feet(max)  \\\n",
       "0                  0.176763            417.388066            933.308089   \n",
       "1                  0.202951            479.225620           1071.581063   \n",
       "2                  0.321655            759.521423           1698.341531   \n",
       "3                  0.012229             28.876199             64.569144   \n",
       "4                  0.176763            417.388066            933.308089   \n",
       "...                     ...                   ...                   ...   \n",
       "4682               0.061290            144.723824            323.612307   \n",
       "4683               0.008460             19.977449             44.670934   \n",
       "4684               0.106510            251.501180            562.373736   \n",
       "4685               0.161210            380.662441            851.187094   \n",
       "4686               0.556661           1314.437764           2939.172192   \n",
       "\n",
       "      epoch_date_close_approach  ...  semi_major_axis  inclination  \\\n",
       "0                  788947200000  ...         1.407011     6.025981   \n",
       "1                  788947200000  ...         1.107776    28.412996   \n",
       "2                  789552000000  ...         1.458824     4.237961   \n",
       "3                  790156800000  ...         1.255903     7.905894   \n",
       "4                  790156800000  ...         1.225615    16.793382   \n",
       "...                         ...  ...              ...          ...   \n",
       "4682              1473318000000  ...         1.161429    39.880491   \n",
       "4683              1473318000000  ...         1.075134     5.360249   \n",
       "4684              1473318000000  ...         1.528234     4.405467   \n",
       "4685              1473318000000  ...         1.486600    21.080244   \n",
       "4686              1473318000000  ...         1.474045    53.574923   \n",
       "\n",
       "      asc_node_longitude  orbital_period  perihelion_distance  perihelion_arg  \\\n",
       "0             314.373913      609.599786             0.808259       57.257470   \n",
       "1             136.717242      425.869294             0.718200      313.091975   \n",
       "2             259.475979      643.580228             0.950791      248.415038   \n",
       "3              57.173266      514.082140             0.983902       18.707701   \n",
       "4              84.629307      495.597821             0.967687      158.263596   \n",
       "...                  ...             ...                  ...             ...   \n",
       "4682          164.183305      457.179984             0.741558      276.395697   \n",
       "4683          345.225230      407.185767             0.996434       42.111064   \n",
       "4684           37.026468      690.054279             0.965760      274.692712   \n",
       "4685          163.802910      662.048343             1.185467      180.346090   \n",
       "4686          187.642183      653.679098             0.876110      222.436688   \n",
       "\n",
       "      aphelion_dist  perihelion_time  mean_anomaly  mean_motion  \n",
       "0          2.005764     2.458162e+06    264.837533     0.590551  \n",
       "1          1.497352     2.457795e+06    173.741112     0.845330  \n",
       "2          1.966857     2.458120e+06    292.893654     0.559371  \n",
       "3          1.527904     2.457902e+06     68.741007     0.700277  \n",
       "4          1.483543     2.457814e+06    135.142133     0.726395  \n",
       "...             ...              ...           ...          ...  \n",
       "4682       1.581299     2.457708e+06    304.306025     0.787436  \n",
       "4683       1.153835     2.458088e+06    282.978786     0.884117  \n",
       "4684       2.090708     2.458300e+06    203.501147     0.521698  \n",
       "4685       1.787733     2.458288e+06    203.524965     0.543767  \n",
       "4686       2.071980     2.458319e+06    184.820424     0.550729  \n",
       "\n",
       "[4687 rows x 33 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "4682    0\n",
       "4683    0\n",
       "4684    0\n",
       "4685    0\n",
       "4686    0\n",
       "Name: hazardous, Length: 4687, dtype: uint8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.5.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (0.17.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (5.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:44:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb=XGBClassifier().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982935153583617"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982935153583617"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"learning_rate\": (0.01, 1.0, \"log-uniform\"),\n",
    "    \"min_child_weight\": (0, 10),\n",
    "    \"max_depth\": (1, 50),\n",
    "    \"max_delta_step\": (0, 10),\n",
    "    \"subsample\": (0.01, 1.0, \"uniform\"),\n",
    "#     \"colsample_bytree\": (0.01, 1.0, \"log-uniform\"),\n",
    "#     \"colsample_bylevel\": (0.01, 1.0, \"log-uniform\"),\n",
    "#     \"reg_lambda\": (1e-9, 1000, \"log-uniform\"),\n",
    "#     \"reg_alpha\": (1e-9, 1.0, \"log-uniform\"),\n",
    "    \"gamma\": (1e-9, 0.5, \"log-uniform\"),\n",
    "    \"min_child_weight\": (0, 5),\n",
    "    \"n_estimators\": (5, 150),\n",
    "#     \"scale_pos_weight\": (1e-6, 500, \"log-uniform\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#bayes_sc =  BayesSearchCV(SVC(),)},n_iter=12,random_state=101).fit(x_train,y_train)\n",
    "bayes_sc=BayesSearchCV(\n",
    "    estimator=xgb,\n",
    "    search_spaces=search_space,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    n_iter=60,\n",
    "    verbose=2,\n",
    "    refit=True,).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "              estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                      colsample_bylevel=1, colsample_bynode=1,\n",
       "                                      colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                                      importance_type='gain',\n",
       "                                      interaction_constraints='',\n",
       "                                      learning_rate=0.300000012,\n",
       "                                      max_delta_step=0, max_depth=6,\n",
       "                                      min_child_weight=1, missing=nan,\n",
       "                                      monoto...\n",
       "                                      scale_pos_weight=1, subsample=1,\n",
       "                                      tree_method='exact',\n",
       "                                      validate_parameters=1, verbosity=None),\n",
       "              n_iter=60, n_jobs=-1, scoring='accuracy',\n",
       "              search_spaces={'gamma': (1e-09, 0.5, 'log-uniform'),\n",
       "                             'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
       "                             'max_delta_step': (0, 10), 'max_depth': (1, 50),\n",
       "                             'min_child_weight': (0, 5),\n",
       "                             'n_estimators': (5, 150),\n",
       "                             'subsample': (0.01, 1.0, 'uniform')},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma', 3.344288616485967e-07),\n",
       "             ('learning_rate', 0.18919982767962126),\n",
       "             ('max_delta_step', 9),\n",
       "             ('max_depth', 18),\n",
       "             ('min_child_weight', 4),\n",
       "             ('n_estimators', 147),\n",
       "             ('subsample', 0.35465500342295286)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_sc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965860597439545"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_sc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes_sc_tuned=BayesSearchCV(\n",
    "#            1learning_rate=1,\n",
    "#            max_delta_step= 8,\n",
    "#            max_depth= 22,\n",
    "#             min_child_weight= 0,\n",
    "#             n_estimators=55,\n",
    "#             subsample= 0.9591413151749281,            \n",
    "#             gamma=(5.507749870519441e-07),#prior='log-uniform')\n",
    "                            \n",
    "#            n_iter=12,\n",
    "#            random_state=101).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bay_pred = bayes_sc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997724039829303"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_sc.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9940273037542662"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_sc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df.iloc[:,:-1]\n",
    "y1=df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train,x1_test,y1_train,y1_test=train_test_split(x1,y1,test_size=0.2,random_state=101,stratify=y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_rs=XGBClassifier().fit(x1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_rs.score(x1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997867803837953"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_rs.score(x1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rs = {'n_estimators':[50,100,150], \n",
    "            'min_samples_split':[8,16],\n",
    "            'min_samples_leaf':[1,2,3,4,5],\n",
    "            'split0_test_score'  : [0.80, 0.84, 0.70],\n",
    "            'split1_test_score'  : [0.82, 0.50, 0.70],\n",
    "            'split0_train_score' : [0.80, 0.92, 0.70],\n",
    "            'split1_train_score' : [0.82, 0.55, 0.70], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.7, split0_test_score=0.84, n_estimators=100, min_samples_split=16, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.7, n_estimators=100, min_samples_split=16, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.82, split1_test_score=0.7, split0_train_score=0.8, split0_test_score=0.7, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2 \n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.5, split0_train_score=0.8, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=2, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.2s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.7, split1_test_score=0.82, split0_train_score=0.7, split0_test_score=0.8, n_estimators=100, min_samples_split=8, min_samples_leaf=5, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.7, n_estimators=50, min_samples_split=8, min_samples_leaf=1, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.8, split0_test_score=0.8, n_estimators=50, min_samples_split=8, min_samples_leaf=3, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.55, split1_test_score=0.82, split0_train_score=0.92, split0_test_score=0.84, n_estimators=100, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.2s\n",
      "[CV] split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4 \n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  split1_train_score=0.82, split1_test_score=0.5, split0_train_score=0.7, split0_test_score=0.84, n_estimators=150, min_samples_split=8, min_samples_leaf=4, total=   0.1s\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   12.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_rs = RandomizedSearchCV(estimator=xgb_rs, \n",
    "                        param_distributions=param_rs,\n",
    "                        #scoring='accuracy',\n",
    "                        cv=10,\n",
    "                        #return_train_score=True,\n",
    "                        verbose=2,\n",
    "                        random_state=101).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split1_train_score': 0.82,\n",
       " 'split1_test_score': 0.5,\n",
       " 'split0_train_score': 0.7,\n",
       " 'split0_test_score': 0.7,\n",
       " 'n_estimators': 50,\n",
       " 'min_samples_split': 8,\n",
       " 'min_samples_leaf': 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, min_samples_split, split0_test_score, split0_train_score, split1_test_score, split1_train_score } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:46:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rs_tuned=XGBClassifier(n_estimators=50,min_samples_split=8,min_samples_leaf=1,split0_test_score=0.7,split0_train_score=0.7,split1_test_score= 0.5,split1_train_score= 0.82).fit(x1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = rs_tuned.predict(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_tuned.score(x1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997867803837953"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_tuned.score(x1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997867803837953"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y1_test, y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "feature_imp=pd.Series(rs_tuned.feature_importances_,index=x_train.columns).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp,y=feature_imp.index)\n",
    "\n",
    "plt.xlabel('scores')\n",
    "plt.ylabel('variables')\n",
    "plt.title('the most influential')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
